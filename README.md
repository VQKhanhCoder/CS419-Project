# Cranfield Document Search Engine

## ğŸ“– Giá»›i thiá»‡u dá»± Ã¡n

Cranfield Document Search Engine lÃ  má»™t há»‡ thá»‘ng tÃ¬m kiáº¿m tÃ i liá»‡u sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Cranfield Collection. Dá»± Ã¡n so sÃ¡nh hiá»‡u suáº¥t cá»§a ba mÃ´ hÃ¬nh Information Retrieval khÃ¡c nhau:

- **Vector Space Model (VSM)**: Sá»­ dá»¥ng TF-IDF vÃ  cosine similarity
- **Latent Semantic Analysis (LSA)**: Ãp dá»¥ng SVD vá»›i k=300 dimensions vÃ  há»— trá»£ Boolean search
- **Whoosh Search Engine**: Sá»­ dá»¥ng thÆ° viá»‡n Whoosh Ä‘á»ƒ full-text search

### ğŸ¯ Má»¥c tiÃªu

- So sÃ¡nh hiá»‡u suáº¥t cÃ¡c mÃ´ hÃ¬nh IR trÃªn Cranfield dataset
- ÄÃ¡nh giÃ¡ theo chuáº©n TREC metrics (MAP, Precision, Recall)
- Cung cáº¥p giao diá»‡n web thÃ¢n thiá»‡n cho ngÆ°á»i dÃ¹ng
- Há»— trá»£ cáº£ free text search vÃ  predefined queries

### ğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡

| Model | MAP Score | Precision | Recall |
|-------|-----------|-----------|---------|
| LSA Model | 0.178 | 0.191 | 0.322 |
| Vector Space Model | 0.158 | 0.182 | 0.308 |
| Whoosh Model | 0.004 | 0.009 | 0.005 |

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
CS419-Project/
â”œâ”€â”€ data/                      # Dataset vÃ  dá»¯ liá»‡u
â”‚   â”œâ”€â”€ Cranfield/            # 1,400 documents (.txt files)
â”‚   â”œâ”€â”€ TEST/                 # Test queries & relevance judgments
â”‚   â”‚   â”œâ”€â”€ query.txt         # 225 test queries
â”‚   â”‚   â””â”€â”€ RES/              # Relevance judgments per query
â”‚   â”œâ”€â”€ Indexing/             # Generated inverted index
â”‚   â”‚   â””â”€â”€ inverted_index.json
â”‚   â””â”€â”€ WhooshIndex/          # Whoosh search index
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/               # Search model implementations
â”‚   â”‚   â”œâ”€â”€ vector_space.py   # Vector Space Model vá»›i TF-IDF
â”‚   â”‚   â”œâ”€â”€ lsa_model.py      # LSA Model vá»›i SVD + Boolean search
â”‚   â”‚   â””â”€â”€ whoosh_model.py   # Whoosh search integration
â”‚   â”œâ”€â”€ preprocessing/        # Text processing modules
â”‚   â”‚   â”œâ”€â”€ preprocessing.py  # Main preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ build_vocab.py    # Vocabulary builder
â”‚   â”‚   â””â”€â”€ vocabulary.json   # Generated vocabulary
â”‚   â”œâ”€â”€ indexing/            # Index building
â”‚   â”‚   â””â”€â”€ inverted_index.py # Inverted index creator
â”‚   â””â”€â”€ evaluation/          # Evaluation system
â”‚       â”œâ”€â”€ evaluator.py     # TREC metrics evaluator
â”‚       â””â”€â”€ results/         # Evaluation results
â”œâ”€â”€ web/                     # Web interface
â”‚   â””â”€â”€ web.py              # Streamlit web application
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ .gitignore              # Git ignore rules
â”œâ”€â”€ run_web.py              # Web app entry point
â””â”€â”€ README.md
```

### ğŸ”§ CÃ¡c thÃ nh pháº§n chÃ­nh

#### 1. Models (`src/models/`)
- **Vector Space Model**: TF-IDF weighting + cosine similarity
- **LSA Model**: Latent Semantic Analysis vá»›i SVD reduction + Boolean operators
- **Whoosh Model**: Full-text search engine integration

#### 2. Preprocessing (`src/preprocessing/`)
- Text normalization vÃ  tokenization vá»›i spaCy
- Stop words removal vÃ  lemmatization
- Vocabulary building tá»« corpus
- Há»— trá»£ cáº£ documents vÃ  queries

#### 3. Indexing (`src/indexing/`)
- Inverted index vá»›i position information
- Efficient storage vÃ  retrieval
- Support cho multiple search models

#### 4. Evaluation (`src/evaluation/`)
- TREC-style evaluation metrics
- Mean Average Precision (MAP)
- Precision-Recall curves
- Per-query performance analysis

#### 5. Web Interface (`web/`)
- Interactive Streamlit application
- Model comparison interface
- Real-time search vÃ  results display
- Support cho Boolean search operators

## ğŸš€ CÃ¡ch cháº¡y chÆ°Æ¡ng trÃ¬nh

### BÆ°á»›c 1: CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
# Clone repository
git clone <repository-url>
cd CS419-Project

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate     # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### BÆ°á»›c 2: Download NLTK data vÃ  spaCy model

```bash
# Download NLTK data
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt_tab'); nltk.download('wordnet'); nltk.download('averaged_perceptron_tagger')"

# Download spaCy English model (this will download ~500MB)
python -m spacy download en_core_web_lg

# Note: Models are downloaded locally and excluded from Git repository
# This prevents large file issues with GitHub
```

### âš ï¸ Important Notes

- **Model Files**: Large ML models (spaCy, NLTK data) are downloaded during setup and excluded from Git
- **Data Files**: Cranfield dataset should be placed in `data/` directory locally  
- **Generated Files**: Indices and caches are created locally and not tracked in Git
- **Cache Directories**: All model caches (transformers, spaCy, etc.) are automatically excluded

### ğŸ—‚ï¸ Local File Structure After Setup

```
CS419-Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Cranfield/           # Place your documents here
â”‚   â””â”€â”€ TEST/               # Place your test queries here
â”œâ”€â”€ .cache/                 # Generated caches (ignored by Git)
â”œâ”€â”€ models/                 # Downloaded models (ignored by Git)
â””â”€â”€ [other project files]
```

### BÆ°á»›c 3: Chuáº©n bá»‹ dá»¯ liá»‡u

```bash
# Build vocabulary tá»« Cranfield documents
cd src/preprocessing
python build_vocab.py

# Táº¡o inverted index
cd ../indexing
python inverted_index.py
```

### BÆ°á»›c 4: Cháº¡y web application

```bash
# Tá»« project root directory
python run_web.py

# Hoáº·c trá»±c tiáº¿p vá»›i Streamlit
streamlit run web/web.py
```

Má»Ÿ browser vÃ  truy cáº­p: `http://localhost:8501`

### BÆ°á»›c 5: Cháº¡y evaluation (tÃ¹y chá»n)

```bash
# ÄÃ¡nh giÃ¡ táº¥t cáº£ models trÃªn test queries
cd src/evaluation
python evaluator.py
```

## ğŸ–¥ï¸ Sá»­ dá»¥ng Web Interface

### 1. Model Selection
- **Vector Space Model**: Traditional TF-IDF approach
- **LSA + Boolean Search**: Semantic search vá»›i Boolean operators

### 2. Search Types
- **Free Text Search**: Nháº­p query tá»± do
- **Predefined Queries**: Chá»n tá»« 225 test queries cÃ³ sáºµn

### 3. Boolean Search (LSA Model)
```
flow AND pressure          # TÃ¬m documents chá»©a cáº£ "flow" vÃ  "pressure"
shock OR wave              # TÃ¬m documents chá»©a "shock" hoáº·c "wave"
boundary NOT layer         # TÃ¬m documents chá»©a "boundary" nhÆ°ng khÃ´ng chá»©a "layer"
(heat AND transfer) OR flow # Combination vá»›i parentheses
```

### 4. Configuration Options
- Number of results (5-50)
- Show/hide document content
- Model-specific parameters

## ğŸ“ˆ Performance & Features

### âœ… Supported Features
- Multiple IR models comparison
- Boolean search operators (LSA model)
- Real-time search results
- TREC-standard evaluation metrics
- Interactive web interface
- Scalable architecture

### ğŸ” Search Capabilities
- **Free text queries**: Natural language input
- **Sample queries**: 225 predefined test queries
- **Boolean operations**: AND, OR, NOT vá»›i LSA model
- **Semantic matching**: Context-aware search vá»›i LSA
- **Relevance scoring**: Confidence scores cho má»—i result

### ğŸ“Š Evaluation Metrics
- **Mean Average Precision (MAP)**
- **Precision at different recall levels**
- **11-point interpolated precision**
- **Per-query performance analysis**

## ğŸ› ï¸ Development & Testing

### Cháº¡y individual components:

```bash
# Test Vector Space Model
cd src/models
python vector_space.py

# Test LSA Model
python lsa_model.py

# Test Whoosh Model
python whoosh_model.py

# Test preprocessing
cd ../preprocessing
python preprocessing.py
```

### Debug vÃ  troubleshooting:
- Check logs trong console output
- Verify data files trong `data/` directory
- Ensure all dependencies Ä‘Æ°á»£c cÃ i Ä‘áº·t Ä‘Ãºng
- Kiá»ƒm tra file paths vÃ  permissions

## ğŸ“ Dataset Information

**Cranfield Collection**:
- 1,400 aerodynamics documents
- 225 test queries
- Ground truth relevance judgments
- Standard IR evaluation benchmark

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Run tests
5. Submit pull request

## ğŸ“„ License

This project is for educational purposes in CS419 Information Retrieval course.
